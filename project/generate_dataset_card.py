"""
Generate a dataset card (Markdown) by merging:

1) Human-written metadata in an optional YAML template (dataset_card.yaml)
2) Measured facts from run artifacts:
   - out/<run>/diagnostics.json
   - out/<run>/corpus.jsonl
3) The config used to generate the run (config_*.yaml), if PyYAML is available

Usage:
  python generate_dataset_card.py \
    --run-dir out/training \
    --config config_training.yaml \
    --output out/training/DATASET_CARD.md \
    --card-yaml dataset_card.yaml

Notes:
- This script intentionally does NOT include a "How to regenerate this card" section in the output.
- This script intentionally does NOT include a "Generated by <script>" field in the output.
"""

from __future__ import annotations

import argparse
import json
import platform
import statistics
import sys
from collections import Counter
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

try:
    import yaml  # type: ignore
except Exception:
    yaml = None


def read_json(path: Path) -> Dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def iter_jsonl(path: Path) -> Iterable[Dict[str, Any]]:
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            yield json.loads(line)


def _percentile(values: List[int], p: int) -> int:
    if not values:
        return 0
    vals = sorted(values)
    k = (len(vals) - 1) * (p / 100.0)
    f = int(k)
    c = min(f + 1, len(vals) - 1)
    if f == c:
        return int(vals[f])
    d0 = vals[f] * (c - k)
    d1 = vals[c] * (k - f)
    return int(round(d0 + d1))


def summarize_corpus(
    corpus_path: Path, max_lines: Optional[int] = None
) -> Dict[str, Any]:
    n = 0
    char_lens: List[int] = []
    doc_type_counts = Counter()
    source_counts = Counter()
    masked_counts = Counter()
    filenames = Counter()

    for obj in iter_jsonl(corpus_path):
        n += 1
        text = obj.get("text", "") or ""
        meta = obj.get("meta", {}) or {}

        char_lens.append(len(text))
        doc_type_counts[str(meta.get("doc_type", "unknown"))] += 1
        source_counts[str(meta.get("source", "unknown"))] += 1
        masked_counts[str(bool(meta.get("pii_masked", False)))] += 1

        fn = meta.get("filename")
        if fn:
            filenames[str(fn)] += 1

        if max_lines is not None and n >= max_lines:
            break

    if n == 0:
        return {
            "lines": 0,
            "char_len": {},
            "top_doc_types": [],
            "top_sources": [],
            "masked_counts": dict(masked_counts),
            "top_filenames": [],
            "sampled": max_lines is not None,
            "max_lines": max_lines,
        }

    char_len = {
        "min": int(min(char_lens)),
        "p50": int(statistics.median(char_lens)),
        "mean": float(statistics.mean(char_lens)),
        "p95": int(_percentile(char_lens, 95)),
        "max": int(max(char_lens)),
        "total": int(sum(char_lens)),
    }

    return {
        "lines": n,
        "char_len": char_len,
        "top_doc_types": doc_type_counts.most_common(10),
        "top_sources": source_counts.most_common(10),
        "masked_counts": dict(masked_counts),
        "top_filenames": filenames.most_common(10),
        "sampled": max_lines is not None,
        "max_lines": max_lines,
    }


def load_yaml(path: Path) -> Dict[str, Any]:
    if yaml is None:
        raise RuntimeError(
            "PyYAML is required to read YAML. Install with: pip install pyyaml"
        )
    return yaml.safe_load(path.read_text(encoding="utf-8")) or {}


def load_optional_yaml(path_str: Optional[str]) -> Dict[str, Any]:
    if not path_str:
        return {}
    p = Path(path_str)
    if not p.exists():
        raise FileNotFoundError(f"Missing YAML: {p}")
    return load_yaml(p)


def format_kv_table(rows: List[Tuple[str, str]]) -> str:
    out = ["| Field | Value |", "|---|---|"]
    for k, v in rows:
        out.append(f"| {k} | {v} |")
    return "\n".join(out)


def format_counts_table(title: str, items: List[Tuple[str, int]]) -> str:
    out = [f"**{title}**", "", "| Value | Count |", "|---|---:|"]
    for k, c in items:
        out.append(f"| {k} | {c} |")
    return "\n".join(out)


def render_owners(owners: Any) -> Optional[str]:
    """
    Render owners list into a human-friendly table.
    Avoids Python-dict-looking output in Markdown previews.
    """
    if not owners:
        return None
    if not isinstance(owners, list):
        return None

    rows: List[Tuple[str, str, str]] = []
    for item in owners:
        if not isinstance(item, dict):
            continue
        name = str(item.get("name", "")).strip()
        aff = str(item.get("affiliation", "")).strip()
        contact = str(item.get("contact", "")).strip()
        if name or aff or contact:
            rows.append((name, aff, contact))

    if not rows:
        return None

    out = ["| Name | Affiliation | Contact |", "|---|---|---|"]
    for name, aff, contact in rows:
        out.append(f"| {name or 'n/a'} | {aff or 'n/a'} | {contact or 'n/a'} |")
    return "\n".join(out)


def enabled_patterns_summary(pii_cfg: Dict[str, Any]) -> List[Tuple[str, str]]:
    patterns = pii_cfg.get("patterns") or {}
    if not isinstance(patterns, dict):
        return []
    rows: List[Tuple[str, str]] = []
    for name, spec in patterns.items():
        if not isinstance(spec, dict):
            continue
        enabled = bool(spec.get("enabled", True))
        if not enabled:
            continue
        hint = "yes" if spec.get("line_hint_regex") else "no"
        rows.append((str(name), f"enabled, line_hint={hint}"))
    return rows


def add_optional_section(lines_md: List[str], title: str, content: Any) -> None:
    if content is None or content == "" or content == [] or content == {}:
        return
    lines_md.append(f"## {title}")
    lines_md.append("")

    if isinstance(content, str):
        lines_md.append(content.strip())
    elif isinstance(content, list):
        for x in content:
            lines_md.append(f"- {str(x).strip()}")
    elif isinstance(content, dict):
        for k, v in content.items():
            if isinstance(v, list):
                lines_md.append(f"**{k}:**")
                for item in v:
                    lines_md.append(f"- {item}")
            else:
                lines_md.append(f"**{k}:** {v}")
    else:
        lines_md.append(str(content))

    lines_md.append("")


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--run-dir", required=True, help="Run directory, e.g. out/training")
    ap.add_argument(
        "--config", required=True, help="Config path, e.g. config_training.yaml"
    )
    ap.add_argument("--output", required=True, help="Output markdown path")
    ap.add_argument(
        "--card-yaml",
        default=None,
        help="Optional dataset_card.yaml with human-written metadata",
    )
    ap.add_argument(
        "--max-lines",
        type=int,
        default=None,
        help="Optional: scan only first N JSONL lines for stats",
    )
    args = ap.parse_args()

    run_dir = Path(args.run_dir)
    cfg_path = Path(args.config)
    out_path = Path(args.output)

    diagnostics_path = run_dir / "diagnostics.json"
    corpus_path = run_dir / "corpus.jsonl"

    if not diagnostics_path.exists():
        raise FileNotFoundError(f"Missing diagnostics: {diagnostics_path}")
    if not corpus_path.exists():
        raise FileNotFoundError(f"Missing corpus: {corpus_path}")
    if not cfg_path.exists():
        raise FileNotFoundError(f"Missing config: {cfg_path}")

    diag = read_json(diagnostics_path)

    cfg: Dict[str, Any] = {}
    if yaml is not None:
        cfg = load_yaml(cfg_path)

    card = load_optional_yaml(args.card_yaml) if args.card_yaml else {}
    corpus_stats = summarize_corpus(corpus_path, max_lines=args.max_lines)

    run_name = str(diag.get("run_name", run_dir.name))
    created_utc = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")

    # Human metadata with defaults
    title = str(card.get("title", f"Dataset Card: {run_name}"))
    subtitle = card.get("subtitle")
    version = str(card.get("version", "n/a"))
    license_str = str(card.get("license", "n/a"))
    languages = card.get("language", [])
    domain = str(card.get("domain", "n/a"))
    dataset_id = str(card.get("dataset_id", f"{run_name}-{run_dir.as_posix()}"))
    lang_str = ", ".join(languages) if isinstance(languages, list) else str(languages)

    # Config knobs (best-effort)
    normalize_cfg = (
        cfg.get("normalize", {}) if isinstance(cfg.get("normalize"), dict) else {}
    )
    clean_cfg = cfg.get("clean", {}) if isinstance(cfg.get("clean"), dict) else {}
    segment_cfg = cfg.get("segment", {}) if isinstance(cfg.get("segment"), dict) else {}
    dedup_cfg = cfg.get("dedup", {}) if isinstance(cfg.get("dedup"), dict) else {}
    pii_cfg = cfg.get("pii_mask", {}) if isinstance(cfg.get("pii_mask"), dict) else {}
    mlx_cfg = (
        cfg.get("mlx_export", {}) if isinstance(cfg.get("mlx_export"), dict) else {}
    )

    counts = diag.get("counts", {}) if isinstance(diag.get("counts"), dict) else {}
    pii_masks = (
        diag.get("pii_masks", {}) if isinstance(diag.get("pii_masks"), dict) else {}
    )
    dedup = diag.get("dedup", {}) if isinstance(diag.get("dedup"), dict) else {}

    lines_md: List[str] = []

    # Header
    lines_md.append(f"# {title}")
    lines_md.append("")
    if subtitle:
        lines_md.append(str(subtitle).strip())
        lines_md.append("")

    # Summary
    lines_md.append("## Summary")
    lines_md.append("")
    lines_md.append(
        "This dataset is a JSONL text corpus produced by a configuration-driven preprocessing pipeline. "
        "It is intended for domain adaptation via continued pretraining with a next-token prediction objective."
    )
    lines_md.append("")

    # Metadata
    lines_md.append("## Metadata")
    lines_md.append("")
    lines_md.append(
        format_kv_table(
            [
                ("Run name", run_name),
                ("Dataset ID", dataset_id),
                ("Version", version),
                ("License", license_str),
                ("Language", lang_str if lang_str else "n/a"),
                ("Domain", domain),
                ("Created (UTC)", created_utc),
            ]
        )
    )
    lines_md.append("")

    owners_md = render_owners(card.get("owners"))
    if owners_md:
        lines_md.append("## Owners and contacts")
        lines_md.append("")
        lines_md.append(owners_md)
        lines_md.append("")

    # Artifacts
    lines_md.append("## Artifacts")
    lines_md.append("")
    lines_md.append(
        format_kv_table(
            [
                ("Config", cfg_path.as_posix()),
                ("Corpus", corpus_path.as_posix()),
                ("Diagnostics", diagnostics_path.as_posix()),
            ]
        )
    )
    lines_md.append("")

    # Narrative sections
    add_optional_section(lines_md, "Intended use", card.get("intended_use"))
    add_optional_section(lines_md, "Out of scope", card.get("out_of_scope"))
    add_optional_section(lines_md, "Data sources", card.get("data_sources"))
    add_optional_section(lines_md, "Sensitive content", card.get("sensitive_content"))
    add_optional_section(
        lines_md, "Preprocessing notes", card.get("preprocessing_notes")
    )
    add_optional_section(lines_md, "Evaluation notes", card.get("evaluation_notes"))

    # Recipe
    lines_md.append("## Preprocessing recipe (from config)")
    lines_md.append("")
    if cfg:
        lines_md.append(
            format_kv_table(
                [
                    (
                        "Normalization",
                        f"collapse_whitespace={normalize_cfg.get('collapse_whitespace', 'n/a')}, remove_boilerplate_lines={normalize_cfg.get('remove_boilerplate_lines', 'n/a')}",
                    ),
                    (
                        "Cleaning",
                        f"min_doc_chars={clean_cfg.get('min_doc_chars', 'n/a')}, max_doc_chars={clean_cfg.get('max_doc_chars', 'n/a')}, max_nonalpha_ratio={clean_cfg.get('max_nonalpha_ratio', 'n/a')}",
                    ),
                    (
                        "Segmentation",
                        f"chunk_chars={segment_cfg.get('chunk_chars', 'n/a')}, overlap_chars={segment_cfg.get('overlap_chars', 'n/a')}, min_segment_chars={segment_cfg.get('min_segment_chars', 'n/a')}, max_segment_chars={segment_cfg.get('max_segment_chars', 'n/a')}",
                    ),
                    ("Deduplication", f"enabled={dedup_cfg.get('enabled', 'n/a')}"),
                    (
                        "PII masking",
                        f"enabled={pii_cfg.get('enabled', 'n/a')}, replacement_format={pii_cfg.get('replacement_format', 'n/a')}",
                    ),
                    (
                        "Training export (MLX)",
                        f"enabled={mlx_cfg.get('enabled', 'n/a')}, dir={mlx_cfg.get('dir','n/a')}, train_frac={mlx_cfg.get('train_frac','n/a')}, seed={mlx_cfg.get('seed','n/a')}",
                    ),
                ]
            )
        )
        lines_md.append("")

        pat_rows = enabled_patterns_summary(pii_cfg)
        if pat_rows:
            lines_md.append("### Enabled masking patterns")
            lines_md.append("")
            lines_md.append("| Pattern | Status |")
            lines_md.append("|---|---|")
            for name, status in pat_rows:
                lines_md.append(f"| {name} | {status} |")
            lines_md.append("")
    else:
        lines_md.append(
            "Config details were not parsed because PyYAML is not installed."
        )
        lines_md.append("")

    # Stats
    lines_md.append("## Dataset size and basic statistics")
    lines_md.append("")
    lines_md.append(
        format_kv_table(
            [
                ("Documents ingested", str(counts.get("documents_ingested", "n/a"))),
                (
                    "Documents after clean",
                    str(counts.get("documents_after_clean", "n/a")),
                ),
                (
                    "Segments after dedup",
                    str(counts.get("segments_after_dedup", "n/a")),
                ),
                ("Dedup removed", str(dedup.get("removed", "n/a"))),
                (
                    "Corpus lines scanned",
                    str(corpus_stats.get("lines", 0))
                    + (" (sampled)" if corpus_stats.get("sampled") else ""),
                ),
            ]
        )
    )
    lines_md.append("")

    if corpus_stats.get("lines", 0) > 0:
        cl = corpus_stats["char_len"]
        lines_md.append("### Text length distribution (characters)")
        lines_md.append("")
        lines_md.append(
            format_kv_table(
                [
                    ("Min", str(cl["min"])),
                    ("Median (p50)", str(cl["p50"])),
                    ("Mean", f"{cl['mean']:.1f}"),
                    ("p95", str(cl["p95"])),
                    ("Max", str(cl["max"])),
                    ("Total characters", str(cl["total"])),
                ]
            )
        )
        lines_md.append("")

        lines_md.append(
            format_counts_table(
                "Top doc_type values (from meta)", corpus_stats["top_doc_types"]
            )
        )
        lines_md.append("")
        lines_md.append(
            format_counts_table(
                "Top source values (from meta)", corpus_stats["top_sources"]
            )
        )
        lines_md.append("")

        lines_md.append("### Masking prevalence (from meta)")
        lines_md.append("")
        mc = corpus_stats["masked_counts"]
        lines_md.append(
            format_kv_table(
                [
                    ("pii_masked=False", str(mc.get("False", 0))),
                    ("pii_masked=True", str(mc.get("True", 0))),
                ]
            )
        )
        lines_md.append("")

    if pii_masks:
        lines_md.append("## PII masking report (from diagnostics)")
        lines_md.append("")
        lines_md.append("| Pattern | Matches |")
        lines_md.append("|---|---:|")
        for k in sorted(pii_masks.keys()):
            lines_md.append(f"| {k} | {pii_masks[k]} |")
        lines_md.append("")

    # Reproducibility (kept minimal, no "Generated by")
    lines_md.append("## Runtime environment")
    lines_md.append("")
    lines_md.append(
        format_kv_table(
            [
                ("Python", sys.version.split()[0]),
                ("Platform", f"{platform.system()} {platform.release()}"),
            ]
        )
    )
    lines_md.append("")

    # Citation block from YAML (optional)
    citation = card.get("citation", {})
    bib = citation.get("recommended_bibtex") if isinstance(citation, dict) else None
    if bib:
        lines_md.append("## Citation")
        lines_md.append("")
        lines_md.append("```bibtex")
        lines_md.append(str(bib).strip())
        lines_md.append("```")
        lines_md.append("")

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("\n".join(lines_md), encoding="utf-8")


if __name__ == "__main__":
    main()
